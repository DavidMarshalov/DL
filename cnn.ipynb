{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86b4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# УКАЖИ СВОИ ПУТИ\n",
    "AI_DIR   = \"/Users/davidmarshalov/Desktop/Мага/ML/архитектура НС/archive/AiArtData/AiArtData\"\n",
    "REAL_DIR = \"/Users/davidmarshalov/Desktop/Мага/ML/архитектура НС/archive/RealArt/RealArt\"\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a972127",
   "metadata": {},
   "source": [
    "В этом файле реализован полный пайплайн классификации изображений (AI-арт vs Real): \n",
    "\n",
    "-загрузка и разбиение данных \n",
    "\n",
    "-аугментации\n",
    "\n",
    "-PyTorch Dataset/DataLoader, обучение собственной компактной CNN \n",
    "\n",
    "-оценка качества по Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d968203",
   "metadata": {},
   "source": [
    "1 - ПОДГОТОВКА ДАТАСЕТА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27846d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено AI: 539\n",
      "Найдено Real: 434\n"
     ]
    }
   ],
   "source": [
    "#rglob - ищет все подряд идухие папки соответсвтующие шаблонуну (все элементы)\n",
    "ai_all   = [p for p in Path(AI_DIR).rglob(\"*\")   if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
    "real_all = [p for p in Path(REAL_DIR).rglob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
    "\n",
    "print(\"Найдено AI:\", len(ai_all))\n",
    "print(\"Найдено Real:\", len(real_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e668ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Годных AI: 539\n",
      "Годных Real: 434\n"
     ]
    }
   ],
   "source": [
    "#проверка на читаемость\n",
    "ai_ok = []\n",
    "for p in ai_all:\n",
    "    try:\n",
    "        Image.open(p).convert(\"RGB\")  \n",
    "        ai_ok.append(p)\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        pass  \n",
    "\n",
    "real_ok = []\n",
    "for p in real_all:\n",
    "    try:\n",
    "        Image.open(p).convert(\"RGB\")\n",
    "        real_ok.append(p)\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        pass\n",
    "\n",
    "print(\"Годных AI:\", len(ai_ok))\n",
    "print(\"Годных Real:\", len(real_ok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "765bdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ai_ok + real_ok\n",
    "y = [1]*len(ai_ok) + [0]*len(real_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29973531",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "270a1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#цеполчка из трансформаций\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #безопаснео кодирование без потери смысла\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "])\n",
    "#Аугментации\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                    # выравниваем масштаб\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # лёгкое кадрирование без потери смысла\n",
    "    transforms.RandomHorizontalFlip(p=0.5),           # зеркалим по горизонтали\n",
    "    transforms.RandomRotation(degrees=10),            # небольшой поворот (камерный «наклон»)\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9202c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset и dataloader \n",
    "\n",
    "#dataset хранит список мутей и меток и знает как читьь и-тую картинку и какие преобразования применитб\n",
    "#dataloader берет из dataset батчи перемешавает и может параллельно  читаь\n",
    "\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "class ImagePathsDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    paths  — список путей (X_train / X_val)\n",
    "    labels — список меток (y_train / y_val), AI=1, Real=0\n",
    "    tfms   — цепочка трансформаций (train_tfms или val_tfms)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, paths, labels, tfms):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.tfms = tfms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        y = self.labels[idx]\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                im = im.convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, OSError) as e:\n",
    "            print(f\"[WARN] Bad image: {p} ({e}) — skipping\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        img = self.tfms(im)\n",
    "        return img, y\n",
    "\n",
    "    \n",
    "train_ds = ImagePathsDataset(X_train, y_train, train_tfms)\n",
    "val_ds   = ImagePathsDataset(X_val,   y_val,   val_tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e14960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "\n",
    "#shuffle - перемешивает порядок в каждую этоху\n",
    "#num_workers - параллельно читает файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "780aac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Реализация неболшой cnn модели с 4 сверточными блоками \n",
    "\n",
    "# ВХОД размером Размер батча * 3* 224 *224\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        # БЛОК 1 3×224×224 → 32×112×112\n",
    "        #3 - сколько входных каналов у данных так как rgb то это 3 \n",
    "        #32 - сколько фильтров на данном шаге собираемся обучить (B, 32, H_out W_out)\n",
    "        #размер окна - 3*3\n",
    "        #паддтнг - 1 - добавление рамки из нулей ширииной 1 чьтбы сохранить размерность и не обрезовать инфу у краев\n",
    "        #параметров - 32*(3*3*3) - 864 (out_channels, in_channels, kernel_h, kernel_w)\n",
    "        #bias - не добавляется смещение так как стоит batchnorm  для экономии\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        #для каждого канала считает по батчку среднее и дисперсию (по осям N,H,W) и нормирует а затем применяет обучаемые параметры( масшаб и сдвиг) по одному каналу\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        #берет макс в акждом непересекабщемся окне 2 на 2\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        # БЛОК 2 32×112×112 → 64×56×56\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # БЛОК 3 64×56×56 → 128×28×28\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # БЛОК 4 128×28×28 → 256×14×14\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4   = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Глобальный усреднительный пуллинг: 256×14×14 → 256×1×1\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # Классификатор (голова)    \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.3), #случайно обнуляет 340 процентов компонент вектора - чтобы ен переобучиться\n",
    "            nn.Linear(256, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)# логит\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.gap(x)                 # усредняет каждую карту 14 на 14 в одно число итого форма (B×256×1×1)\n",
    "        x = x.view(x.size(0), -1)       # B×256\n",
    "        logits = self.head(x)           # B×1\n",
    "        return logits.squeeze(1)        # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4c9278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#лосс, оптимизатор\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #выбор устройства вычислений\n",
    "model = CNN().to(device) #создаем модель и переносим ее на выбранное устрйосвтво\n",
    "\n",
    "# функция потерь для бинарной классификации. Ожидает логиты (сырые и внутри применяет сигмоиду + бинаруню кросс энтропию\n",
    "#так как кассы несбалансированны - задаю штраф за ошибки по редкому классу\n",
    "N_pos = 539\n",
    "N_neg = 434\n",
    "pos_weight = torch.tensor([N_neg / N_pos], device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72a5d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    1) модель в режим обучения\n",
    "    2) по батчам: прямой проход → лосс → backward → шаг оптимизатора\n",
    "    3) возвращаем средний train loss за эпоху\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.float32, device=device)\n",
    "\n",
    "        optimizer.zero_grad()#обнуляю градиенты перед новым шагом\n",
    "        logits = model(imgs)                 # прямой проход на выходе (B,) - сырые логиты\n",
    "        loss = criterion(logits, labels)     # считаем лосс по батчу (среднее по батчу)\n",
    "        loss.backward() #обратый проход\n",
    "        optimizer.step() #обновляем параметры по накопленным градиентам\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    1) модель в режим eval (замораживает Dropout/BN статистики)\n",
    "    2) считаем средний val loss\n",
    "    3) собираем предсказания → переводим в вероятности через сигмоиду\n",
    "    4) считаем метрики: Accuracy / Precision / Recall\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.float32, device=device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits).numpy()   # (N,)\n",
    "    all_labels = torch.cat(all_labels).numpy()   # (N,)\n",
    "    probs = 1 / (1 + np.exp(-all_logits))        # сигмоида\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    acc  = accuracy_score(all_labels, preds)\n",
    "    prec = precision_score(all_labels, preds, zero_division=0)\n",
    "    rec  = recall_score(all_labels, preds, zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader.dataset), acc, prec, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9a5504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.4351 | val_loss=0.5889 | acc=0.7231 | prec=0.8068 | rec=0.6574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_loss=0.4159 | val_loss=0.5575 | acc=0.7231 | prec=0.7935 | rec=0.6759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train_loss=0.3971 | val_loss=0.5853 | acc=0.6821 | prec=0.7255 | rec=0.6852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train_loss=0.3926 | val_loss=0.6200 | acc=0.6872 | prec=0.7423 | rec=0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train_loss=0.4086 | val_loss=0.6412 | acc=0.6718 | prec=0.6618 | rec=0.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train_loss=0.4416 | val_loss=0.5451 | acc=0.6872 | prec=0.7238 | rec=0.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train_loss=0.4182 | val_loss=0.5819 | acc=0.7077 | prec=0.7684 | rec=0.6759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train_loss=0.3906 | val_loss=0.7572 | acc=0.6205 | prec=0.8696 | rec=0.3704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | train_loss=0.4158 | val_loss=0.5494 | acc=0.7333 | prec=0.7979 | rec=0.6944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train_loss=0.4243 | val_loss=0.6171 | acc=0.7179 | prec=0.7573 | rec=0.7222\n",
      "\n",
      "=== ЛУЧШАЯ МОДЕЛЬ ===\n",
      "Epoch: 6\n",
      "val_loss: 0.5451\n",
      "Accuracy: 0.6872\n",
      "Precision: 0.7238\n",
      "Recall: 0.7037\n",
      "Сохранено в: cnn_best.pth\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10  \n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "best_metrics = None\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # === train ===\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.float32, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)                      \n",
    "        loss = criterion(logits, labels)         \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.float32, device=device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "\n",
    "    all_logits = torch.cat(all_logits).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    probs = 1 / (1 + np.exp(-all_logits))        \n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    val_loss = val_total_loss / len(val_loader.dataset)\n",
    "    acc  = accuracy_score(all_labels, preds)\n",
    "    prec = precision_score(all_labels, preds, zero_division=0)\n",
    "    rec  = recall_score(all_labels, preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
    "          f\"acc={acc:.4f} | prec={prec:.4f} | rec={rec:.4f}\")\n",
    "\n",
    "    # === save best by val_loss ===\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        best_metrics = {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"val_loss\": val_loss}\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "# === restore best, save to disk, print best metrics ===\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    torch.save(model.state_dict(), \"cnn_best.pth\")\n",
    "\n",
    "    print(\"\\n=== ЛУЧШАЯ МОДЕЛЬ ===\")\n",
    "    print(f\"Epoch: {best_epoch}\")\n",
    "    print(f\"val_loss: {best_metrics['val_loss']:.4f}\")\n",
    "    print(f\"Accuracy: {best_metrics['acc']:.4f}\")\n",
    "    print(f\"Precision: {best_metrics['prec']:.4f}\")\n",
    "    print(f\"Recall: {best_metrics['rec']:.4f}\")\n",
    "    print(\"Сохранено в: cnn_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c63651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модель показала средний уровень качества — точность около 68–73%, при этом precision примерно 0.72 и примерно recall ≈ 0.70 и говорят, \n",
    "#что сеть относительно хорошо различает классы, но всё ещё путается в части примеров.\n",
    "#\n",
    "#Скорее модель видит базовые различия но требуется тонкая настройка, возможно поможет усиление аугментации (что повысит обощающую способность мб)\n",
    "#добавить по больше dropout чтобы модеь не переобучалась\n",
    "\n",
    "####################### Буду рад вашим предложениям и рекомендациям по улучшению модели. ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789efad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9352529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25332d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea9ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4aa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef6849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92149a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c00b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da609e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
