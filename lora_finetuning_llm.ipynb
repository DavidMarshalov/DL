{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIOKPZGbyNWX"
      },
      "source": [
        "В данном блокноте будет реализована задача дообучения LLM  с помощью метода LoRa на датасете Dahoas/sft-gptj-synthetic-prompt-responses.\n",
        "\n",
        "В датасете около 44 тысяч пар «prompt–response»: в столбце prompt лежит задание или вопрос, который подаётся на вход модели, а в столбце response — желаемый ответ, который мы будем считать таргетом.\n",
        "\n",
        "Цель - взять относительно небольшую уже предобученную языковую модель, добавить к её слоям LoRA-адаптеры, обучить только эти дополнительные параметры на данном датасете и в итоге получить модель, которая по новому prompt сможет генерировать ответ, максимально похожий на целевой response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vqcKUUHP3Lwr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uk2eEOXw7Dr",
        "outputId": "7452385e-ba9a-44b4-e9dd-5c89a08a2206"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade transformers accelerate datasets bitsandbytes peft trl sentencepiece einops evaluate\n",
        "import torch, platform, os, transformers, datasets, peft, bitsandbytes, trl\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available?', torch.cuda.is_available())\n",
        "!nvidia-smi || true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cu7IaHm3OdM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR0gmYlIw7Gc",
        "outputId": "e20ccb39-abf7-4a07-c124-601db07791ae"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    #Базовая модель\n",
        "    base_model: str = \"gpt2\" #TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    dataset_name: str = \"Dahoas/sft-gptj-synthetic-prompt-responses\"\n",
        "    subset_size: int = 10000        # чтобы обучение шло быстрее\n",
        "\n",
        "    # QLoRA (4-бит) или обычная LoRA\n",
        "    use_qlora: bool = True         # True = QLoRA, False = LoRA без квантования\n",
        "\n",
        "    #Гиперпараметры LoRA\n",
        "    lora_r: int = 16 #размер скрытого пространства lora\n",
        "    lora_alpha: int = 32 #для мастабирования вклада lora адаптеров\n",
        "    lora_dropout: float = 0.05\n",
        "\n",
        "    #Гиперпараметры обучения\n",
        "    max_steps: int = 300\n",
        "    per_device_train_batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 8\n",
        "    learning_rate: float = 2e-4\n",
        "    warmup_ratio: float = 0.05\n",
        "\n",
        "    output_dir: str = \"outputs-lora-gptj\"\n",
        "    eval_prompts: list = None\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "if cfg.eval_prompts is None:\n",
        "    cfg.eval_prompts = [\n",
        "        \"Explain what LoRA fine-tuning is in simple words.\",\n",
        "        \"Give me three ideas for a weekend hobby for a data scientist.\",\n",
        "        \"Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday.\",\n",
        "        \"What is the best way to save money to start a business?\"\n",
        "    ]\n",
        "\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWpPQn497O9N"
      },
      "source": [
        "Токенайзер и базовая модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r2ekQNI37Z59"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "956b1608dbf6407584345da850b7ae71",
            "814cb73372c545aa8027cefebac1f132",
            "2623b19c0f834fc89dc6f95a952b859b",
            "787ff1ae63f04281bef3e3e68f0cb403",
            "d854a2f7e9f14ab4bcd7be1f8d16df9d",
            "ff8b73b870de4142b2aed911d4742ef4",
            "dbcbfa641f374b4eb011bde87b83a69a",
            "9abfcf798a2b4a629cf1d84f6bcdacb5",
            "8d3524c7a7cd4a078bbca8856073ccbd",
            "5da370b1ff57479fb4e12ff860770a94",
            "a2746d2de5ba4f44b682c5a4d797d194",
            "cb8a1ae119804e8c9acc1c62c2fd0a66",
            "f13ae324db5b45ff90ae437f51310233",
            "ec86160f770742688e8c4422018ede7f",
            "9a0ffb7770dc4225bdc2b14e2b7554a9",
            "47b44489f45b4280a311902f3be32505",
            "b0d8e91bcf03475b881edd9b0bd62931",
            "7a671db2ee9a44c8a146ba57586e9a7a",
            "5fe86ad51c384837a63311ea49bb91f8",
            "c04b4099b926404284c7821471872a89",
            "d60cf6b9e24d4f1080fb622630388fe5",
            "e281a60cb16b4d34a90cf1c20b4a35d6",
            "492d662828eb4c929a001cc8b54a20a7",
            "69710bfe465e45a5a7aa8f44a22a0563",
            "2bf13852bb57447ab42fa9940a562e92",
            "7454e3d9053340f9bf80b4169375aa8d",
            "c52bafff0d664668b9afaaaf10fb852e",
            "51231f5e2c2841c48fb841d606e22b2d",
            "1a170d9d65f24526a12b7d851e3be2b8",
            "e136a13518624a42a54fcd9bbcb9134f",
            "c092cbdcaa0043c0b368522a50a9efc2",
            "6f39c6833e2f40d4b5d09258a6b94e97",
            "f21d9458364e48129b524312d7743fc0",
            "0fc5f531352a4591bf90021ce39df1ee",
            "d540e0f4b7db418abdf5f0944ec0d14f",
            "e15d826a8e8a4e28bac81f9983c64567",
            "14a38fd6e866444d9a49bd7fb6018374",
            "3c42a8bb08724ee596d497cc0621537d",
            "6afccb77fca14be79e6ac64d4d17368b",
            "fab02db3bc1043d39a9e0ea59ccf4ba2",
            "56dec11d8107423ea3256e701bfbe891",
            "52aaa81d63414ae1bf48c361fa4f3d98",
            "3335125809a444f39d8d9759560ef488",
            "6457f0b6696f4c56b1183dabacc2b945",
            "02d78337c0874f13a79fac37a3494709",
            "b839e40ebfe54fd2aa259e05cdac39ac",
            "199f116a77234cb797dd24c6c6f663eb",
            "9d2a09a644a1447eb3bb136cb6d130a9",
            "f8a63a74d78446b1aec816acebdeb8ac",
            "455bdb19c6574ab5b10b7f81e8028cb5",
            "32f6c91b4c3145fdbf8d02a796f91e9f",
            "5c3130aba058498caccf866ff20b885d",
            "43127ffa7be14267bfc93627d3eaa7f8",
            "896614bc07f343098193cf3aa50f1246",
            "9ae665b275de48be9fd097af24d4b239"
          ]
        },
        "id": "OWj-qHnYw7Iy",
        "outputId": "8102b519-27a2-47a4-81ce-a83f307c20ab"
      },
      "outputs": [],
      "source": [
        "# токенайзер\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=True)\n",
        "tokenizer.padding_side = \"right\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# тип чисел\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "bnb_config = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G9btOlbONh5n"
      },
      "outputs": [],
      "source": [
        "#!pip install -U bitsandbytes accelerate transformers datasets peft trl\n",
        "cfg.use_qlora = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "5b80089968774a3b9756331aa2689501",
            "69dc712c96b64de6baa4dd1f677c8aa5",
            "19dee2ddad3e4b8d9e6750e4bbc39331",
            "3af83c4729a84fd4a53f23699a89f83e",
            "98e522a3ec00476c98d6e55aa8c63ea4",
            "46e601c60d034549b8fe04d2a1c29c3f",
            "e3264ea9a15e4fa19674f9ad7ca408f7",
            "6094ac300d6f478fb36d19cb7ec1efac",
            "a2d7e6fe73c04206b4daeeca16694d93",
            "16db906dd36744fcb6b13bd3169b90a9",
            "fa684d890d864c55892b4b35d8bdc913",
            "09d0d258e8fe44e8b7cf9d82b4d21c1b",
            "aad63d2db617403f88ec2e1f37a9ade9",
            "f40b2baa43c14d9386ff1bd67edfb7b1",
            "b4bbe3ea36134458abc68f7633f214de",
            "5e01927240434d3496ae5f58566ba49a",
            "2b8ae8f6983e48f889f624077a7b9a28",
            "f37ab53e890d4c6f834738b3d7a709fe",
            "68da44902a6e43d18be768367a37723d",
            "515926b9723a4a4a896e49d093318b7a",
            "0e69fbffb560420ba1e01dedd6cbe3b8",
            "3ab566b9a9364c418d8a0494483578cf"
          ]
        },
        "id": "IhWvEHU8Dz_Q",
        "outputId": "d5727f69-2180-42e9-bff9-be8eaee1dae1"
      },
      "outputs": [],
      "source": [
        "if cfg.use_qlora:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=dtype,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        cfg.base_model,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        cfg.base_model,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "print(\"Loaded model:\", cfg.base_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1uudk9Bw7Le"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujJ6OKyE6wQ"
      },
      "source": [
        "Проверка базовых ответов модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ree4asc5b08"
      },
      "outputs": [],
      "source": [
        "#pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fZPjUPv5E67_"
      },
      "outputs": [],
      "source": [
        "from textwrap import indent\n",
        "import torch\n",
        "import re\n",
        "\n",
        "import inspect\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "40A-ZsEr5Ti_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jfjpfcF7w7N5"
      },
      "outputs": [],
      "source": [
        "def generate_base(prompt, max_new_tokens=200, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "    with torch.no_grad(): #просто делаем предсказание ничего не обучаем\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        generated_tokens = outputs[0, input_len:]\n",
        "\n",
        "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqEssbXpEKEs",
        "outputId": "9bc0f089-dac7-46ff-e121-2248c2dc42a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Explain what LoRA fine-tuning is in simple words.',\n",
              " 'Give me three ideas for a weekend hobby for a data scientist.',\n",
              " \"Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday.\",\n",
              " 'What is the best way to save money to start a business?']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cfg.eval_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qqVA_toUKf1Z"
      },
      "outputs": [],
      "source": [
        "pre_eval = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LLfx5yJhEH7P"
      },
      "outputs": [],
      "source": [
        "pre_eval['Explain what LoRA fine-tuning is in simple words.'] = generate_base('Explain what LoRA fine-tuning is in simple words.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4u-dVZXXEQ_C"
      },
      "outputs": [],
      "source": [
        "pre_eval['Give me three ideas for a weekend hobby for a data scientist.'] = generate_base('Give me three ideas for a weekend hobby for a data scientist.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GUWjjy0eERBX"
      },
      "outputs": [],
      "source": [
        "pre_eval[\"Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday.\"] = generate_base(\"Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L7wqMpSiEREF"
      },
      "outputs": [],
      "source": [
        "pre_eval['What is the best way to save money to start a business?'] = generate_base('What is the best way to save money to start a business?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufUw0b9856US",
        "outputId": "4547d71e-895c-4142-b18e-c0131056a735"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Explain what LoRA fine-tuning is in simple words.': \"\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'Yes-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\\n\\n* - Check for a 'No-Padding' checkbox if there is no padding\",\n",
              " 'Give me three ideas for a weekend hobby for a data scientist.': '\\n\\n1. Connect with the Data Scientist and build your skills using data science.\\n\\n2. Create a data science project that includes all the tools needed to create your own data science project.\\n\\n3. Create a data science team of data scientists and data scientists in your own data science teams.\\n\\nThe data scientist can use data science for any type of data science project.\\n\\nYou will have to be prepared for every project.\\n\\nDo not underestimate the power of data science.\\n\\nData science is a high skill set.\\n\\nIt is your best friend.',\n",
              " \"Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday.\": \" Alice: who does a lot of this stuff? Bob: that's a really good question. Alice: I think it's really nice that she's one of my favorite characters. Bob: all the way up to the very end of the story. Alice: yeah. Bob: you should see those pictures. You should see them. Alice: I wanna see them. Bob: they're all too good. Alice: I think the picture is pretty pretty. Bob: they're pretty good. Alice: I should try to get your mind off of this stuff, Bob. Bob: how much do you like seeing it? Alice: Yeah, I should probably get up to that. Bob: this is a really good question. Alice: well, I'm not sure I have a better answer. Bob: you've never been to a movie, Alice. Bob: but you should probably see some pictures of this. Alice: you know, it's a great question. Bob: it's a\",\n",
              " 'What is the best way to save money to start a business?': ' How do I make it easier for my employees to join?\\n\\nWith the right tools, you can help save money by helping others. The following tips will help you start a business in your own time.\\n\\n1. Start a business online.\\n\\nIf you are an entrepreneur, you are better off using a website like Startups.com. You will not only get instant access to a variety of information, but you will get the best deals. You will get to know the best companies and products for you and are able to find the best deals for you. One of the best ways to start a business online is to check out an online booking service such as Booking.com.\\n\\n2. Create your own website.\\n\\nCreate a website is usually the first step in your business plan. However, if you are not sure how to start a business online, then you can help yourself by creating a website. You can use a free website like Business.com to'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P-05pWbD8fW",
        "outputId": "634902cd-1ec4-411b-850e-78afc35a6b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASELINE RESPONSES ===\n",
            "\n",
            "# Prompt:\n",
            " Explain what LoRA fine-tuning is in simple words.\n",
            "\n",
            "# Response (baseline):\n",
            " \n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'Yes-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "# Prompt:\n",
            " Give me three ideas for a weekend hobby for a data scientist.\n",
            "\n",
            "# Response (baseline):\n",
            " \n",
            "\n",
            "  1. Connect with the Data Scientist and build your skills using data science.\n",
            "\n",
            "  2. Create a data science project that includes all the tools needed to create your own data science project.\n",
            "\n",
            "  3. Create a data science team of data scientists and data scientists in your own data science teams.\n",
            "\n",
            "  The data scientist can use data science for any type of data science project.\n",
            "\n",
            "  You will have to be prepared for every project.\n",
            "\n",
            "  Do not underestimate the power of data science.\n",
            "\n",
            "  Data science is a high skill set.\n",
            "\n",
            "  It is your best friend.\n",
            "\n",
            "# Prompt:\n",
            " Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday.\n",
            "\n",
            "# Response (baseline):\n",
            "    Alice: who does a lot of this stuff? Bob: that's a really good question. Alice: I think it's really nice that she's one of my favorite characters. Bob: all the way up to the very end of the story. Alice: yeah. Bob: you should see those pictures. You should see them. Alice: I wanna see them. Bob: they're all too good. Alice: I think the picture is pretty pretty. Bob: they're pretty good. Alice: I should try to get your mind off of this stuff, Bob. Bob: how much do you like seeing it? Alice: Yeah, I should probably get up to that. Bob: this is a really good question. Alice: well, I'm not sure I have a better answer. Bob: you've never been to a movie, Alice. Bob: but you should probably see some pictures of this. Alice: you know, it's a great question. Bob: it's a\n",
            "\n",
            "# Prompt:\n",
            " What is the best way to save money to start a business?\n",
            "\n",
            "# Response (baseline):\n",
            "    How do I make it easier for my employees to join?\n",
            "\n",
            "  With the right tools, you can help save money by helping others. The following tips will help you start a business in your own time.\n",
            "\n",
            "  1. Start a business online.\n",
            "\n",
            "  If you are an entrepreneur, you are better off using a website like Startups.com. You will not only get instant access to a variety of information, but you will get the best deals. You will get to know the best companies and products for you and are able to find the best deals for you. One of the best ways to start a business online is to check out an online booking service such as Booking.com.\n",
            "\n",
            "  2. Create your own website.\n",
            "\n",
            "  Create a website is usually the first step in your business plan. However, if you are not sure how to start a business online, then you can help yourself by creating a website. You can use a free website like Business.com to\n"
          ]
        }
      ],
      "source": [
        "#pre_eval = {}\n",
        "#for p in cfg.eval_prompts:\n",
        "#    pre_eval[p] = generate_base(p)\"\n",
        "\n",
        "print(\"=== BASELINE RESPONSES ===\")\n",
        "for p, out in pre_eval.items():\n",
        "    print(\"\\n# Prompt:\\n\", p)\n",
        "    print(\"\\n# Response (baseline):\\n\", indent(out, \"  \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf0ZdE2QDGcj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "fc9714673a154cebb9f69067a386bbf7",
            "9b8022253e3e4b7e81a69494bdffb34d",
            "86d3ac25f08a408aad501bc8c4d1c581",
            "648a8c069b854f4d8f8fc0f8eaa0841f",
            "2578a5c3b8c04a7fb131329fab6554d3",
            "56e67088b1f141ba944b295521c8dd41",
            "751106e480044018acafde6f51c2b170",
            "112fc88777664c3a90e016f908480da4",
            "14ab7c18a0d641c2b932cb27d7054ece",
            "b17322aca6a448019384dd2f0ba4a536",
            "280b5956677a4f5b826cac966acff264",
            "72e2fa6240c5461e906cd6fad81258bf",
            "7996792d037646b9a459ee57136a2617",
            "a75dabfe76364d2a9b8d9eb078b9e941",
            "62d9b69619894057b19cd9a03d7cd020",
            "a46f730a8aa94d77b406ed70e11032a6",
            "85622290733e4217a43013acde8ea00a",
            "f25104b3b78b4711a7db439b21d9e9ed",
            "593af4ccef1043ae85eb8493cf2c8f52",
            "d851293fd3dc4f3f9e988d6aad7fd598",
            "851b84ce13e848378a39356d0f9de30b",
            "fcf9c0824f2c49b1ad3daafab08bae6c",
            "3ddf47f6c4db434bb96ddf658dc1d6f9",
            "0a5016d601d444d58fb6bc2df00df055",
            "e03f119246704852aa5bcef565a649e3",
            "7aff13dcd3a5419c9c96ce72ad736058",
            "e6f1654f6d7b48a8bcadb0d290c8c1e8",
            "68359eb9f6c44e7f83be69fd7b8e985e",
            "49790f1fd3d44e2587abd223eef35472",
            "6159527799ee4c2b9c44a3e9badcc344",
            "8104d7c06f1f4e4dbd8404d0269e9bfb",
            "b7dceb7044154bc9a26bf2eae4b09329",
            "603fe341294d4cffb94f4cb54cc49c0e"
          ]
        },
        "id": "k51XDD3rw7Qa",
        "outputId": "d5f9fb97-2e70-4808-d068-fd6d44d985ec"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(cfg.dataset_name)\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "1a36efb87f0a4bbd821e7413c9feba6e",
            "1b525c00d72a4663ab284e0416d0ae79",
            "50593901ce3745cc8f9c26be92af325c",
            "dbf0d62a4810456c8cbbc656c083f5f7",
            "56b9f4dec05d4940ae83b4c49266a049",
            "4a8aa125ba584873873bfc1b766a7d3c",
            "d7408bf7ca034e878e88d785348ed08f",
            "26aa483f20a0428d883db699da532caa",
            "66f97bfdd1fd4165b4b70955a779f3d6",
            "aaee8879d4bc4acf82d1ec26f36be79e",
            "47b0c65d60c2411b9a162a9022a729e1"
          ]
        },
        "id": "bM_wDAlVw7S5",
        "outputId": "3af0b85c-0ba2-4770-db2c-8bb726a4559f"
      },
      "outputs": [],
      "source": [
        "def format_example(ex):\n",
        "    # в этом датасете уже есть поля prompt и response\n",
        "    prompt = ex[\"prompt\"]\n",
        "    completion = ex[\"response\"]\n",
        "\n",
        "    # чуть-чуть чистим\n",
        "    prompt = re.sub(r\"\\s+$\", \"\", prompt) + \"\\n\"   # убираем пробелы в конце + ровно один \\n\n",
        "    completion = completion.lstrip()             # убираем пробелы в начале\n",
        "\n",
        "    return {\"prompt\": prompt, \"completion\": completion}\n",
        "\n",
        "def prepare_split(dataset, split=\"train\"):\n",
        "    data = dataset[split]\n",
        "    # берём подвыборку, чтобы было быстрее (можно потом увеличить)\n",
        "    if cfg.subset_size and len(data) > cfg.subset_size:\n",
        "        data = data.shuffle(seed=42).select(range(cfg.subset_size))\n",
        "    mapped = data.map(format_example, remove_columns=data.column_names)\n",
        "    return mapped\n",
        "\n",
        "train_ds = prepare_split(ds, \"train\")\n",
        "print(train_ds[0])\n",
        "print(\"Train size:\", len(train_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62urJHYyKIB-"
      },
      "source": [
        "Настройка LoRa и SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204,
          "referenced_widgets": [
            "5467ed7acb8f46b0b4a10f0484de013d",
            "a91ca707084f41758a191d9e432d5a2d",
            "52e133d838a24924bff93abbbc3a2ac8",
            "7547a869536c4527a18bfa4cb639f357",
            "fb7a038f70954aa09d91369c95ca3d36",
            "53fffabf9cb24b85a836604980a664ce",
            "efb692ec588243bf8c5dfc0de7addb61",
            "42711469ac604d66966dc1335c7963db",
            "098c8f61ad26411e893acd143dfa9239",
            "29a2a90775d04fa491562ba85631c3fe",
            "6f41cc3823db4cd8b0128ee438535f61",
            "dc524d316fe6471b81065df1bec5d528",
            "4857445cc8c2444b83cb49620e50e570",
            "7326897dc7224779b49b5447a07e912a",
            "39cbb85a69524629a5744ebbcba81d63",
            "ea5e5fddd37a4ac5a533c897c235f261",
            "19cb0faa19004994a82a225d53cc9ac0",
            "e3ba125d645d4f6a81b75580c8b46a15",
            "f065cffa4ea8422b81a93e33080aba2c",
            "9dccd16319654b0f9eac8589b58fe857",
            "0b7f6622b8b049dc8b14bc9020679d56",
            "581a8d57a40c4f6fbd91febbe5abbda1",
            "d107606f7c944f2f9e250f933845f9be",
            "cdbd6deb171f47329f46fd3919933147",
            "4de1d5dc70214aebbe20ab8e1c4b3a5d",
            "5ef2da3ff7ee4799b82eb300ea3fa421",
            "9b414560b2044215a673ed6713887d6c",
            "e17ba3651a8340c8a7a768990a43674e",
            "f89dd29ea73e48f69f9cc31748575534",
            "6e7f50a5fd8c448bb06ee4ec75064f0a",
            "41da10a37fcc4f7e9481987c33cba48b",
            "2636bca2775d400891da67d6cceb0d9e",
            "7826a144d32c47c1a78cf15ab9b7d5a6"
          ]
        },
        "id": "zy2CRXOYw7YM",
        "outputId": "ddd07ba1-8643-4f62-add8-6d4a4a890211"
      },
      "outputs": [],
      "source": [
        "# В какие слои модели вешаем LoRA-адаптеры\n",
        "#target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "#target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "target_modules = [\"c_attn\", \"c_fc\", \"c_proj\"]\n",
        "\n",
        "# LoRA-конфиг\n",
        "peft_config = LoraConfig(\n",
        "    r=cfg.lora_r,\n",
        "    lora_alpha=cfg.lora_alpha,\n",
        "    lora_dropout=cfg.lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "# Конфиг обучения\n",
        "sftconf_kwargs = dict(\n",
        "    output_dir=cfg.output_dir,\n",
        "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    logging_steps=10,\n",
        "    max_steps=cfg.max_steps,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    fp16=not cfg.use_qlora,               # если НЕ QLoRA — используем fp16\n",
        "    bf16=True if cfg.use_qlora else False,\n",
        "    optim=\"paged_adamw_8bit\" if cfg.use_qlora else \"adamw_torch\",\n",
        "    gradient_checkpointing=True,\n",
        "    packing=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "args = SFTConfig(**sftconf_kwargs)\n",
        "\n",
        "# Создаём SFTTrainer\n",
        "trainer_kwargs = dict(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=train_ds,\n",
        ")\n",
        "\n",
        "trainer_sig = inspect.signature(SFTTrainer.__init__)\n",
        "if \"processing_class\" in trainer_sig.parameters:\n",
        "    trainer_kwargs[\"processing_class\"] = tokenizer\n",
        "elif \"tokenizer\" in trainer_sig.parameters:\n",
        "    trainer_kwargs[\"tokenizer\"] = tokenizer\n",
        "if \"dataset_text_field\" in trainer_sig.parameters:\n",
        "    trainer_kwargs[\"dataset_text_field\"] = None\n",
        "\n",
        "trainer = SFTTrainer(**trainer_kwargs)\n",
        "\n",
        "# Смотрим, сколько параметров реально обучаем\n",
        "trainable = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in trainer.model.parameters())\n",
        "#print(f\"Trainable: {trainable:,} / Total: {total:,} ({100*trainable/total:.3f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d1gT_ixaw7ar",
        "outputId": "d8845a25-4f16-4ce3-8972-24b4da95c711"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 1:59:32, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.197600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.069800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.083600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.874300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.816700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.724900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.670300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.637700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.498400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.584300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.668000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.593700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.533500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.504000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.582400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.590600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.509500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.596900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.462600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.399900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.517500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.472300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.588500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.541600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.593400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.464500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=300, training_loss=2.633639367421468, metrics={'train_runtime': 7232.6899, 'train_samples_per_second': 0.332, 'train_steps_per_second': 0.041, 'total_flos': 161015828281344.0, 'train_loss': 2.633639367421468, 'epoch': 0.24})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_result = trainer.train()\n",
        "train_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFR2oeVvWRL7"
      },
      "source": [
        "Сравнение ответов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iZkFTg_WOxQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jNTvG8Qjw7iy"
      },
      "outputs": [],
      "source": [
        "trainer.model.eval()\n",
        "trainer.model.config.use_cache = True\n",
        "\n",
        "if cfg.use_qlora:\n",
        "    autocast_dtype = torch.bfloat16\n",
        "else:\n",
        "    autocast_dtype = torch.bfloat16   # можно fp16, если удобно\n",
        "\n",
        "def generate_finetuned(prompt, max_new_tokens=128, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n",
        "    with torch.no_grad():\n",
        "        if torch.cuda.is_available() and autocast_dtype in (torch.float16, torch.bfloat16):\n",
        "            with torch.autocast(\"cuda\", dtype=autocast_dtype):\n",
        "                out = trainer.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=temperature,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                    eos_token_id=tokenizer.eos_token_id,\n",
        "                )\n",
        "        else:\n",
        "            out = trainer.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x5m28KagVL__"
      },
      "outputs": [],
      "source": [
        "post_eval = {}\n",
        "for p in cfg.eval_prompts:\n",
        "    post_eval[p] = generate_finetuned(p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UktDBAYLXgvr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVpwyvW-w7oB",
        "outputId": "47a1e1cf-78f6-4b68-bb86-d258e88ce065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== POST-TUNING RESPONSES ===\n",
            "\n",
            "# Prompt:\n",
            " Explain what LoRA fine-tuning is in simple words.\n",
            "\n",
            "# Before:\n",
            " \n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'Yes-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "  * - Check for a 'No-Padding' checkbox if there is no padding\n",
            "\n",
            "# After:\n",
            "   Explain what LoRA fine-tuning is in simple words. \n",
            "  LoRA are a group of professionals, who do a lot of work in the legal community, and they are very interested in interpreting the law, and then interpreting the law in a way that is consistent with the law.  They think that the law should be interpreted accurately, but they think that it is important that the law is clear and that everyone understands what is being said. \n",
            "  LoRA are not an official organization, but are just friends.  They are not a legal organization, as they are not a legal person, but they share the same interest in the legal profession, and they are a community with a lot\n",
            "\n",
            "# Prompt:\n",
            " Give me three ideas for a weekend hobby for a data scientist.\n",
            "\n",
            "# Before:\n",
            " \n",
            "\n",
            "  1. Connect with the Data Scientist and build your skills using data science.\n",
            "\n",
            "  2. Create a data science project that includes all the tools needed to create your own data science project.\n",
            "\n",
            "  3. Create a data science team of data scientists and data scientists in your own data science teams.\n",
            "\n",
            "  The data scientist can use data science for any type of data science project.\n",
            "\n",
            "  You will have to be prepared for every project.\n",
            "\n",
            "  Do not underestimate the power of data science.\n",
            "\n",
            "  Data science is a high skill set.\n",
            "\n",
            "  It is your best friend.\n",
            "\n",
            "# After:\n",
            "   Give me three ideas for a weekend hobby for a data scientist.  I think you can do two of these:\n",
            "  -Go for a walk with your friends and make an infographic.  These are often a good idea for getting a feel for what you want to do, and what you're looking for.  This kind of activity is very valuable for your research, and people can learn a lot from it.\n",
            "  -Learn about statistics, and get some practice with them.  They can help you make more informed decisions, and they make the decisions that you need.  They can help you keep track of your activity, and make you more productive.\n",
            "\n",
            "# Prompt:\n",
            " Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday.\n",
            "\n",
            "# Before:\n",
            "    Alice: who does a lot of this stuff? Bob: that's a really good question. Alice: I think it's really nice that she's one of my favorite characters. Bob: all the way up to the very end of the story. Alice: yeah. Bob: you should see those pictures. You should see them. Alice: I wanna see them. Bob: they're all too good. Alice: I think the picture is pretty pretty. Bob: they're pretty good. Alice: I should try to get your mind off of this stuff, Bob. Bob: how much do you like seeing it? Alice: Yeah, I should probably get up to that. Bob: this is a really good question. Alice: well, I'm not sure I have a better answer. Bob: you've never been to a movie, Alice. Bob: but you should probably see some pictures of this. Alice: you know, it's a great question. Bob: it's a\n",
            "\n",
            "# After:\n",
            "   Summarize this dialogue in one sentence: Alice: how's the status? Bob: shipped v1 yesterday, patch coming Monday. Alice:  what? Bob:   How? Alice:    What? Bob:    What? Alice:    What? Bob:    What? Alice:     What? Bob:    What? Alice:    What? Bob:    What? Alice:    How? Alice:    What? Bob:    What? Alice:    How?\n",
            "\n",
            "\n",
            "\n",
            "# Prompt:\n",
            " What is the best way to save money to start a business?\n",
            "\n",
            "# Before:\n",
            "    How do I make it easier for my employees to join?\n",
            "\n",
            "  With the right tools, you can help save money by helping others. The following tips will help you start a business in your own time.\n",
            "\n",
            "  1. Start a business online.\n",
            "\n",
            "  If you are an entrepreneur, you are better off using a website like Startups.com. You will not only get instant access to a variety of information, but you will get the best deals. You will get to know the best companies and products for you and are able to find the best deals for you. One of the best ways to start a business online is to check out an online booking service such as Booking.com.\n",
            "\n",
            "  2. Create your own website.\n",
            "\n",
            "  Create a website is usually the first step in your business plan. However, if you are not sure how to start a business online, then you can help yourself by creating a website. You can use a free website like Business.com to\n",
            "\n",
            "# After:\n",
            "   What is the best way to save money to start a business?\n",
            "  There are many different ways to save money, including investing in a business, building a business, and buying good products.  For example, you could invest in a business that is not a business, and you could invest in a business that is not a business, and you could invest in an online business and you could invest in a business that is not a business, and you could invest in a business that is not a business, and you could invest in a business that is not a business, and you could invest in a business that is not a business, and you could invest in an online business and you could invest in a\n"
          ]
        }
      ],
      "source": [
        "print(\"=== POST-TUNING RESPONSES ===\")\n",
        "from textwrap import indent\n",
        "for p in cfg.eval_prompts:\n",
        "    print(\"\\n# Prompt:\\n\", p)\n",
        "    print(\"\\n# Before:\\n\", indent(pre_eval[p], \"  \"))\n",
        "    print(\"\\n# After:\\n\", indent(post_eval[p], \"  \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwWBXpQ8ZadQ"
      },
      "source": [
        "После дообучения с помощью LoRA модель стала генерировать ответы более осмысленно и ближе к формату ответа, чем до обучения, но по качеству видно, что результат все еще далек от адекватного ответа llm: до обучения модель практически давала бессвязный текст, а после — начала пытаться выдавать связанный ответ по теме и пытаться объяснять, хоть и без точного содержания (например, интерпретирует LoRA как юридическую организацию или повторяет части текста). Это говорит о том, что эффект обучения заметен — модель стала лучше реагировать на задачу и формат prompt–response, но из-за маленького размера модели и ограниченного количества обучаемых параметров качество ответов остаётся низким: модель приобрела направление ответа, но не сформировала корректную семантику.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dPIJXNOTw7qr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJpTQXGDZaEX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I0NNgtkZaGm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
   
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
